# ==========================================
# üì¶ INSTALL DEPENDENCIES
# ==========================================
print("üì¶ Installing required packages...")
!pip install requests pandas tqdm beautifulsoup4 lxml fake-useragent python-dotenv tldextract --quiet
print("‚úÖ Packages installed successfully\n")

# ==========================================
# üî• IMPORT LIBRARIES
# ==========================================
print("üìö Importing libraries...")
import requests
import pandas as pd
import time
from tqdm import tqdm
from urllib.parse import urlparse, urljoin, urlunparse
from bs4 import BeautifulSoup
from fake_useragent import UserAgent
import re
import tldextract
import getpass
import urllib3
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)
print("‚úÖ Libraries imported\n")

# ==========================================
# üõ† USER INPUTS
# ==========================================
print("üìù Please provide required inputs\n")

industries_input = input("üè≠ Enter industry for which Leads Required. \n Ex:Cable Tray manufacturers,\nPre Engineered manufacturers,\nSolar Mounting structure manufacturers").strip()

cities_input = input("üìç Enter city: ").strip()

SERPER_API_KEY = getpass.getpass("üîë Enter your Serper API Key: ").strip()
DEEPSEEK_API_KEY = getpass.getpass("üîë Enter your LLM API Key: ").strip()



industries = [i.strip() for i in industries_input.split(",") if i.strip()]
cities = [c.strip() for c in cities_input.split(",") if c.strip()]

if not industries or not cities or not SERPER_API_KEY or not DEEPSEEK_API_KEY:
    raise ValueError("‚ùå All inputs are required")

print("\nüöÄ Starting lead generation pipeline...")
print(f"üîç Industries selected: {industries}")
print(f"üìç Cities selected: {cities}")
print("‚è≥ This may take some time depending on number of websites\n")

# ==========================================
# 1Ô∏è‚É£ SERPER GOOGLE SEARCH
# ==========================================
def get_serper_results(query, total_results=10):
    all_results = []
    url = "https://google.serper.dev/search"
    headers = {"X-API-KEY": SERPER_API_KEY, "Content-Type": "application/json"}
    pages = (total_results + 9) // 10

    for page in range(1, pages + 1):
        try:
            payload = {"q": query, "num": 10, "page": page}
            response = requests.post(url, headers=headers, json=payload, timeout=30)
            data = response.json()
            if "organic" in data:
                all_results.extend(data["organic"])
        except Exception as e:
            print(f"‚ùå Error for query '{query}' (page {page}): {e}")
        time.sleep(1.5)
    return all_results[:total_results]

print("üåê Step 1: Researching online for company websites...\n")

all_data = []
seen_links = set()

for city in tqdm(cities, desc="Processing cities"):
    print(f"\nüìç Working on city: {city}")
    for industry in industries:
        print(f"   üè≠ Searching industry: {industry}")
        query = f"{industry} companies in {city} -indiamart -justdial"
        results = get_serper_results(query)
        for idx, res in enumerate(results, start=1):
            link = res.get("link", "").strip()
            if not link or link in seen_links:
                continue
            seen_links.add(link)
            all_data.append({
                "industry": industry,
                "city": city,
                "search_query": query,
                "title": res.get("title", ""),
                "website": link,
                "snippet": res.get("snippet", ""),
                "displayed_link": res.get("displayed_link", link)
            })

df_websites = pd.DataFrame(all_data)
df_websites.to_csv("lead_generation_results.csv", index=False)

print(f"\n‚úÖ Step 1 Complete: {len(df_websites)} unique websites collected")
print("üìÅ File saved: lead_generation_results.csv\n")

# ==========================================
# 2Ô∏è‚É£ EXTRACT PARENT LINKS
# ==========================================
print("üîó Step 2: Extracting main (parent) website links...")

def get_parent_link(website):
    try:
        parsed = urlparse(website)
        return f"{parsed.scheme}://{parsed.netloc}"
    except:
        return ""

df_websites['parent_link'] = df_websites['website'].apply(get_parent_link)

# ==========================================
# 3Ô∏è‚É£ REMOVE DUPLICATES
# ==========================================
df_websites = df_websites.drop_duplicates(subset=['parent_link']).reset_index(drop=True)
print(f"‚úÖ Step 2 Complete: {len(df_websites)} parent links after deduplication\n")

# ==========================================
# 4Ô∏è‚É£ EXTRACT RAW HTML TEXT
# ==========================================
print("üìÑ Step 3: Extracting raw text from Link...")

ua = UserAgent()

def smart_get(url, retries=3):
    if not isinstance(url, str) or not url.strip():
        return "Error: Invalid or empty URL"
    url = url.strip()
    if not url.startswith(("http://", "https://")):
        url = "https://" + url
    for attempt in range(retries):
        try:
            headers = {
                "User-Agent": ua.random,
                "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8",
                "Accept-Language": "en-US,en;q=0.9",
            }
            response = requests.get(url, timeout=12, headers=headers, allow_redirects=True, verify=False)
            if response.status_code == 200:
                return response.text
            if response.status_code in [403, 406, 429, 500, 502]:
                time.sleep(2 + attempt)
                continue
            return f"Error: HTTP {response.status_code}"
        except Exception as e:
            if attempt == retries - 1:
                return f"Error: {e}"
            time.sleep(1)
    return "Error: Max retries failed"

def extract_text(url):
    html = smart_get(url)
    if html is None or str(html).startswith("Error"):
        return html
    try:
        soup = BeautifulSoup(html, "lxml")
        for tag in soup(["script", "style", "noscript", "iframe"]):
            tag.decompose()
        return soup.get_text(" ", strip=True)
    except:
        return "Error parsing HTML"

df_websites["Extracted_Raw_Text"] = df_websites['parent_link'].apply(extract_text)
print("‚úÖ Step 3 Complete: Website text extracted\n")

# ==========================================
# 5Ô∏è‚É£ EXTRACT COMPANY NAME VIA DEEPSEEK
# ==========================================
print("üè∑ Step 4: Extracting company names using AI...")

def extract_company_name(url, html_text):
    endpoint = "https://api.deepseek.com/v1/chat/completions"
    prompt = f"Extract ONLY the company name from the following website content.\nWebsite URL: {url}\nHTML Page Text:\n{html_text}\nReturn ONLY the company name. No explanations."
    headers = {"Content-Type": "application/json", "Authorization": f"Bearer {DEEPSEEK_API_KEY}"}
    payload = {"model": "deepseek-chat", "messages": [{"role": "user", "content": prompt}], "temperature": 0}
    try:
        response = requests.post(endpoint, headers=headers, json=payload)
        response.raise_for_status()
        result = response.json()
        return result["choices"][0]["message"]["content"].strip()
    except:
        return "ERROR"

df_websites["Company_Name"] = df_websites.apply(
    lambda row: extract_company_name(row["parent_link"], row["Extracted_Raw_Text"]), axis=1
)
print("‚úÖ Step 4 Complete: Company names extracted\n")

# ==========================================
# 6Ô∏è‚É£ EXTRACT COMPANY PROFILE
# ==========================================
print("üßæ Step 5: Extracting company profiles (About Us)...")

HEADERS = {'User-Agent': 'Mozilla/5.0'}

def extract_visible_text_clean(html):
    soup = BeautifulSoup(html, 'lxml')
    for tag in soup(['script','style','noscript','header','footer','form','svg','nav']):
        tag.decompose()
    return ' '.join(soup.stripped_strings)

def find_about_page(base_url):
    try:
        res = requests.get(base_url, headers=HEADERS, timeout=10)
        soup = BeautifulSoup(res.text, 'lxml')
        return [urljoin(base_url, l['href']) for l in soup.find_all('a', href=True)
                if any(k in l['href'].lower() for k in ['about','profile','company'])]
    except:
        return []

def extract_company_profile(url):
    pages = find_about_page(url)
    for p in pages:
        try:
            r = requests.get(p, headers=HEADERS, timeout=10)
            text = extract_visible_text_clean(r.text)
            if len(text) > 100:
                return text
        except:
            pass
    return ""

df_websites["Company_Profile"] = df_websites['parent_link'].apply(extract_company_profile)
print("‚úÖ Step 5 Complete: Company profiles extracted\n")

# ==========================================
# 7Ô∏è‚É£ CLASSIFY RELEVANCY VIA DEEPSEEK
# ==========================================
print("üß† Step 6: Classifying companies for relevancy with AI...")

DEEPSEEK_API_URL = "https://api.deepseek.com/chat/completions"

def classify_company(name, profile, industry):
    prompt = f"You are a B2B industry classification expert.\nCompany Name: {name}\nCompany Profile: {profile}\nQuestion: Is this company primarily involved in {industry} activities that require STEEL? Answer YES / NO / UNCLEAR only."
    payload = {"model": "deepseek-chat","messages":[{"role":"user","content":prompt}],"temperature":0}
    headers = {"Authorization": f"Bearer {DEEPSEEK_API_KEY}","Content-Type":"application/json"}
    try:
        r = requests.post(DEEPSEEK_API_URL, headers=headers, json=payload, timeout=30)
        return r.json()["choices"][0]["message"]["content"].strip().upper()
    except:
        return "ERROR"

df_websites["Relevancy"] = df_websites.apply(
    lambda r: classify_company(r["Company_Name"], r["Company_Profile"], r["industry"]),
    axis=1
)

print("‚úÖ Step 6 Complete: Relevancy classification done\n")

# ==========================================
# 8Ô∏è‚É£ EXTRACT CONTACT DETAILS
# ==========================================
print("üìû Step 7: Collating phone numbers and email IDs...")

def find_contact_pages(base_url):
    try:
        r = requests.get(base_url, headers=HEADERS, timeout=10)
        soup = BeautifulSoup(r.text, 'lxml')
        return [urljoin(base_url,l['href']) for l in soup.find_all('a', href=True)
                if any(k in l['href'].lower() for k in ['contact','support','reach'])]
    except:
        return []

def extract_contacts_from_page(url):
    phones, emails = set(), set()
    try:
        r = requests.get(url, headers=HEADERS, timeout=12)
        text = r.text
        phones.update(re.findall(r'[6-9]\d{9}', text))
        emails.update(re.findall(r'[\w\.-]+@[\w\.-]+\.\w+', text))
    except:
        pass
    return phones, emails

all_contacts = []

for _, row in tqdm(df_websites.iterrows(), total=len(df_websites), desc="Extracting contacts"):
    base = row['parent_link']
    p,e = extract_contacts_from_page(base)
    for page in find_contact_pages(base):
        p2,e2 = extract_contacts_from_page(page)
        p.update(p2); e.update(e2)
    all_contacts.append({
        "Website": base,
        "Phone Numbers": ", ".join(p),
        "Email Addresses": ", ".join(e)
    })

df_contacts = pd.DataFrame(all_contacts)
df_final = pd.merge(
    df_websites,
    df_contacts,
    left_on="parent_link",
    right_on="Website",
    how="left"
)
df_final.to_csv("B2B_Lead_Full_Output.csv", index=False)

print("\nüéâ PIPELINE COMPLETE!")
print("üìÅ Final file saved as: B2B_Lead_Full_Output.csv")
print("‚úÖ You can now download and use the leads")
